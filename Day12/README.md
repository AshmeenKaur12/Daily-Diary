# Day 12 — Neural Networks

**Date:** July 08, 2025

---

## Overview

On **Day 12**, I studied **Neural Networks**, one of the core building blocks of Deep Learning.  
I explored how they mimic the human brain’s interconnected neurons to learn patterns, make predictions, and solve complex tasks.

---

## Key Concepts

- **Neurons:** Basic units that receive and process inputs.
- **Connections:** Links between neurons with adjustable weights and biases.
- **Layers:**
  - **Input Layer:** Receives raw data.
  - **Hidden Layers:** Transform data through learned patterns.
  - **Output Layer:** Produces final predictions (e.g., class labels).

---

## How They Work

1️ **Forward Propagation:**  
- Input → Linear transformation → Activation function → Output prediction.

2️ **Loss Calculation:**  
- Compare predicted vs actual output using a loss function.

3️ **Backpropagation:**  
- Compute gradients.
- Update weights/biases using optimization algorithms (e.g., Stochastic Gradient Descent).

4️ **Iteration:**  
- Repeat forward + backward passes over many epochs to minimize loss.

---

## Types of Neural Networks

-  **Feedforward Neural Network**
-  **Single-layer Perceptron**
-  **Multilayer Perceptron (MLP)**
-  **Convolutional Neural Network (CNN)**
-  **Recurrent Neural Network (RNN)**
-  **Long Short-Term Memory (LSTM)**

---

## File

- `Day12.pdf` — Complete detailed notes on Neural Networks.

---

## Author

**Name:** Ashmeen Kaur  
**URN:** 2302486  
**CRN:** 2315030

---

> **"Neural Networks imitate how humans learn — layer by layer, connection by connection."**

